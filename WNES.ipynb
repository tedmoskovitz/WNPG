{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as multivariate_normal\n",
    "sns.set()\n",
    "import gym\n",
    "import simpleenvs # a custom mujoco env \n",
    "from kwng_es import *\n",
    "from es_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_wassdist(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, params: dict) -> None:\n",
    "        super(random_wassdist, self).__init__()\n",
    "        self.sigma = 1.0 \n",
    "        self.gamma = 1.0 \n",
    "        self.T = 10\n",
    "        self.n = 1 # num data points per batch to update kernel function\n",
    "        self.xdim = params['dim']\n",
    "        self.alpha = 5e-2\n",
    "        self.D = params['rf_dim'] # random feature dimension\n",
    "        standard_normal = torch.distributions.normal.Normal(0, 1)\n",
    "        self.omega = standard_normal.sample((self.D, self.xdim)) * 1.0 / self.sigma\n",
    "        self.bias = standard_normal.sample((self.D, 1)) * 2 * np.pi\n",
    "        self.t = 0\n",
    "        \n",
    "    def get_random_feature(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: tensor of shape [xdim, ]\n",
    "        \"\"\"\n",
    "        if type(x) != torch.Tensor: x = torch.tensor(x, dtype=torch.float32); \n",
    "        return torch.cos(torch.matmul(self.omega, x) + self.bias) * np.sqrt(2. / self.D)\n",
    "    \n",
    "    def update(self, X: torch.Tensor, Y: torch.Tensor, params: dict) -> None:\n",
    "        \"\"\"\n",
    "        sample and update behavioral test functions using single_update\n",
    "        \"\"\"\n",
    "        for _ in range(params['w_iter']):\n",
    "            x = X[int(np.random.uniform() * len(X))]\n",
    "            y = Y[int(np.random.uniform() * len(X))]\n",
    "            self.single_update(x, y)\n",
    "        \n",
    "    def single_update(self, x: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        update behavioral test functions\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)\n",
    "        y = y.unsqueeze(1)\n",
    "        if self.t == 0:\n",
    "            # initialize the functions\n",
    "            self.beta_1 = self.get_random_feature(x).view(-1)\n",
    "            self.beta_2 = self.get_random_feature(y).view(-1)\n",
    "        else:\n",
    "            # map to random feature space\n",
    "            zx = self.get_random_feature(x)\n",
    "            zy = self.get_random_feature(y)\n",
    "            \n",
    "            C = torch.sum((x - y).pow(2), axis=0)\n",
    "            coeff = torch.exp((torch.matmul(self.beta_1, zx) - torch.matmul(self.beta_2, zy) - C) / self.gamma)\n",
    "            weight = torch.tensor([1 - coeff])\n",
    "            weight = weight.unsqueeze(0)\n",
    "            \n",
    "            # update the functions\n",
    "            self.beta_1 += self.alpha * torch.mean(weight * zx, axis=1)\n",
    "            self.beta_2 += self.alpha * torch.mean(weight * zy, axis=1)\n",
    "        self.t += 1\n",
    "        \n",
    "    def wd(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        compute the Wasserstein distance\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)\n",
    "        y = y.unsqueeze(1)\n",
    "        \n",
    "        zx = self.get_random_feature(x)\n",
    "        zy = self.get_random_feature(y)\n",
    "        beta1_zx = torch.matmul(self.beta_1, zx)\n",
    "        beta2_zy = torch.matmul(self.beta_2, zy)\n",
    "        wd = beta1_zx - beta2_zy \\\n",
    "            + (1 / self.gamma) \\\n",
    "            * torch.exp((beta1_zx - beta2_zy - torch.sum((x - y).pow(2))) / self.gamma) \n",
    "        return wd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toeplitz(c: torch.Tensor, r: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    (slow) pytorch implementation of scipy's toeplitz function\n",
    "    args:\n",
    "        c: the first column of the toeplitz matrix\n",
    "        r: the first row; note that differences in the top left entry \n",
    "            default to the first entry of the first column\n",
    "    outputs:\n",
    "        the result toeplitz matrix\n",
    "    \"\"\"\n",
    "    c = c.view(-1); r = r.view(-1); \n",
    "    num_rows = len(c); num_cols = len(r); \n",
    "    m = torch.zeros([num_rows, num_cols], dtype=torch.float32)\n",
    "    m[0, :] = r\n",
    "    m[:, 0] = c\n",
    "    \n",
    "    # the final num_cols - 1 entries in row r are the first \n",
    "    # num_cols - 1 entries of the row above\n",
    "    # this results in a matrix with constant diagonals\n",
    "    for r in range(1, num_rows):\n",
    "        m[r, 1:] = m[r-1, :-1]\n",
    "    return m\n",
    "\n",
    "class ToeplitzPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, args: dict) -> None:\n",
    "        super(ToeplitzPolicy, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.init_seed = args['seed']\n",
    "        self.ob_dim = args['ob_dim']\n",
    "        self.h_dim = args['h_dim']\n",
    "        self.ac_dim = args['ac_dim']\n",
    "        \n",
    "        self.w1 = nn.Parameter(\n",
    "            self.weight_init(self.ob_dim + self.h_dim -1, args['zeros'])\n",
    "        )\n",
    "        \n",
    "        self.b1 = nn.Parameter(\n",
    "            self.weight_init(self.h_dim, args['zeros'])\n",
    "        )\n",
    "\n",
    "        self.w2 = nn.Parameter(\n",
    "            self.weight_init(self.h_dim * 2 - 1, args['zeros'])\n",
    "        )\n",
    "\n",
    "        self.b2 = nn.Parameter(\n",
    "            self.weight_init(self.h_dim, args['zeros'])\n",
    "        )\n",
    "        self.w3 = nn.Parameter(\n",
    "            self.weight_init(self.ac_dim + self.h_dim - 1, args['zeros'])\n",
    "        )\n",
    "        \n",
    "        self.W1 = self.build_layer(self.h_dim, self.ob_dim, self.w1)\n",
    "        self.W2 = self.build_layer(self.h_dim, self.h_dim, self.w2)\n",
    "        self.W3 = self.build_layer(self.ac_dim, self.h_dim, self.w3)\n",
    "\n",
    "        self.params = torch.cat([self.w1, self.b1, self.w2, self.b2, self.w3])\n",
    "        self.N = len(self.params)\n",
    "        \n",
    "    \n",
    "    def weight_init(self, d: int, zeros: bool) -> torch.Tensor:\n",
    "        if zeros: w = torch.zeros(d, device=self.device, requires_grad=True); \n",
    "        else:\n",
    "            torch.manual_seed(self.init_seed)\n",
    "            w = torch.rand(d, device=self.device, requires_grad=True) / np.sqrt(d)\n",
    "        return (w)\n",
    "    \n",
    "    def build_layer(self, d1: int, d2: int, v: torch.Tensor) -> torch.Tensor:\n",
    "        # len v = d1 + d2 - 1\n",
    "        col = v[:d1]\n",
    "        row = v[(d1-1):]\n",
    "        \n",
    "        W = toeplitz(col, row) # sketchy \n",
    "        return (W)\n",
    "    \n",
    "    \n",
    "    def update(self, vec: torch.Tensor) -> None:\n",
    "        \"\"\"add vector of updates to parameters\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.params += vec\n",
    "\n",
    "            self.w1 += vec[:len(self.w1)]\n",
    "            vec = vec[len(self.w1):]\n",
    "            self.b1 += vec[:len(self.b1)]\n",
    "            vec = vec[len(self.b1):]\n",
    "            self.w2 += vec[:len(self.w2)]\n",
    "            vec = vec[len(self.w2):]\n",
    "            self.b2 += vec[:len(self.b2)]\n",
    "            vec = vec[len(self.b2):]\n",
    "            self.w3 += vec\n",
    "\n",
    "            self.W1 = self.build_layer(self.h_dim, self.ob_dim, self.w1)\n",
    "            self.W2 = self.build_layer(self.h_dim, self.h_dim, self.w2)\n",
    "            self.W3 = self.build_layer(self.ac_dim, self.h_dim, self.w3)\n",
    "\n",
    "    \n",
    "    def __call__(self, X: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        z1 = torch.tanh(torch.matmul(self.W1, X) + self.b1)\n",
    "        z2 = torch.tanh(torch.matmul(self.W2, z1) + self.b2)\n",
    "        return (torch.tanh(torch.matmul(self.W3, z2)))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class worker(object):\n",
    "    \n",
    "    def __init__(self, params: dict, master: ToeplitzPolicy, A: torch.Tensor, i: int, train: bool = True) -> None:\n",
    "        self.env = gym.make(params['env_name'])\n",
    "        self.v = A[i, :] # the perturbation we will use\n",
    "\n",
    "        params['zeros'] = True # initialize policy with zeros so we can set it to the current policy\n",
    "        self.policy = ToeplitzPolicy(params)\n",
    "        self.policy.update(master.params)\n",
    "        self.timesteps = 0\n",
    "        \n",
    "    def do_rollouts(self, seed: int = 0, train: bool = True) -> None:\n",
    "        \n",
    "        self.policy.update(self.v) # Add the perturbation, to calculate F(theta + sigma * epsilon)\n",
    "        up, up_data = self.rollout(seed, train)\n",
    "        \n",
    "        self.policy.update(-2 * self.v) # Subtract the perturbation, to calculate F(theta - sigma * epsilon)\n",
    "        down, down_data = self.rollout(seed, train)\n",
    "        \n",
    "        self.rewards = torch.tensor([up, down]).view(2)\n",
    "        self.up_data = up_data\n",
    "        self.down_data = down_data\n",
    "    \n",
    "    def rollout(self, seed: int = 0, train: bool = True) -> tuple:\n",
    "        self.env.seed(0)\n",
    "        state = self.env.reset()\n",
    "        self.env._max_episode_stesp = 400 \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        data = []\n",
    "        while not done:\n",
    "            action = self.policy(torch.tensor(state, dtype=torch.float32))\n",
    "            action_dist = multivariate_normal(action, 0.01 * torch.eye(action.numel()))\n",
    "            action = action_dist.sample((1,))\n",
    "            state, reward, done, _ = self.env.step(action.numpy())\n",
    "            total_reward += reward\n",
    "            data.append([state, reward])\n",
    "            self.timesteps += 1\n",
    "        return (total_reward, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(data: list) -> torch.Tensor:\n",
    "    # reward to go - used for ant-wall env\n",
    "    rewards = [x[1] for x in data]\n",
    "    d = pd.DataFrame({'Traj': rewards[::-1]})\n",
    "    embedding = torch.tensor(d.cumsum().Traj.values, dtype=torch.float32)\n",
    "    return(embedding)\n",
    "\n",
    "def calcdists(embeddings: list, dists: torch.Tensor, i: int, master: ToeplitzPolicy,\n",
    "              m_embedding: torch.Tensor, params: dict):\n",
    "    \n",
    "    n_master = min(params['n_iter'], params['n_prev'])\n",
    "\n",
    "    if n_master== 1:\n",
    "        dists[i, 0] = master.wass.wd(m_embedding[0], embeddings[i])\n",
    "        dists[i, 1] = master.wass.wd(m_embedding[0], embeddings[i+params['num_sensings']])\n",
    "    else:\n",
    "        # if we are comparing vs multiple previous policies\n",
    "        dists[i, 0] = torch.mean(torch.tensor([master.wass.wd(x, embeddings[i]) for x in m_embedding]))\n",
    "        dists[i, 1] = torch.mean(torch.tensor([master.wass.wd(x, embeddings[i+params['num_sensings']]) for x in m_embedding]))\n",
    "    return(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rollouts(master: ToeplitzPolicy, A: torch.Tensor, params: dict) -> tuple:\n",
    "       \n",
    "    all_rollouts = torch.zeros([params['num_sensings'], 2])\n",
    "    up, down = [],[]\n",
    "    timesteps = 0\n",
    "    \n",
    "    for i in range(params['num_sensings']):\n",
    "        w = worker(params, master, A, i)\n",
    "        w.do_rollouts()\n",
    "        all_rollouts[i] = w.rewards\n",
    "        up.append(embed(w.up_data))\n",
    "        down.append(embed(w.down_data))\n",
    "        timesteps += w.timesteps\n",
    "\n",
    "    embeddings = up + down\n",
    "    \n",
    "    if params['optimizer'] == 'ES':\n",
    "        dists = torch.zeros(params['num_sensings'])\n",
    "    else:\n",
    "        # Update behavioral test funcs and use them to calculate WDs for each perturbed policy\n",
    "        if params['n_iter'] == 1:\n",
    "            dists = torch.zeros(params['num_sensings'])\n",
    "        else:\n",
    "            dists = torch.zeros([params['num_sensings'], 2])\n",
    "            master.wass.update(master.buffer, embeddings, params)\n",
    "            for i in range(params['num_sensings']):\n",
    "                dists = calcdists(embeddings, dists, i, master, master.embedding, params)\n",
    "\n",
    "            # normalize dists\n",
    "            dists = (dists - torch.mean(dists)) / (torch.std(dists)  + 1e-8)     \n",
    "            dists = dists[:, 0] - dists[:, 1]\n",
    "        master.buffer = embeddings\n",
    "    \n",
    "    # normalize rewards    \n",
    "    all_rollouts = (all_rollouts - torch.mean(all_rollouts)) / (torch.std(all_rollouts)  + 1e-8)  \n",
    "    # compute R_k - R_t\n",
    "    m = all_rollouts[:, 0] - all_rollouts[:, 1]\n",
    "    \n",
    "    return(m, dists, torch.stack(embeddings), timesteps)\n",
    "\n",
    "\n",
    "class ES_criterion(nn.Module):\n",
    "\n",
    "    def __init__(self, beta=0.):\n",
    "        \"\"\"\n",
    "        the ES loss function\n",
    "        \"\"\"\n",
    "        super(ES_criterion, self).__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, perturbed_rewards, wdists, sample_params, param_dist):\n",
    "        F = (1 - self.beta) * perturbed_rewards + self.beta * wdists\n",
    "        return F.mean()\n",
    "\n",
    "def ES(params: dict, master: ToeplitzPolicy) -> tuple:\n",
    "    \n",
    "    A_dist = multivariate_normal(torch.zeros(master.N), torch.eye(master.N))\n",
    "    A = A_dist.sample((params['num_sensings'],))\n",
    "    A *= params['sigma']\n",
    "    A /= torch.norm(A, dim=-1).view(-1, 1)\n",
    "        \n",
    "    noisy_rewards, wdists, outputs, timesteps = aggregate_rollouts(master, A, params)\n",
    "    \n",
    "    g = torch.zeros(master.N)\n",
    "    for i in range(params['num_sensings']):\n",
    "        eps = A[i, :]\n",
    "        # combine reward and WD\n",
    "        g += eps * ((1 - params['beta']) * noisy_rewards[i] + params['beta'] * wdists[i]) # *\n",
    "    \n",
    "    g /= (2 * params['sigma'])\n",
    "    \n",
    "    return(g, noisy_rewards, wdists, outputs, A, A_dist, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam(dx, m, v, learning_rate, t, eps = 1e-8, beta1 = 0.9, beta2 = 0.999):\n",
    "    m = beta1 * m + (1 - beta1) * dx\n",
    "    mt = m / (1 - beta1 ** t)\n",
    "    v = beta2 * v + (1-beta2) * (dx.pow(2))\n",
    "    vt = v / (1 - beta2 **t)\n",
    "    update = learning_rate * mt / (torch.sqrt(vt) + eps)\n",
    "    return(update, m, v)\n",
    "\n",
    "def SGD(dx, m, v, learning_rate, t):\n",
    "    return learning_rate * dx\n",
    "\n",
    "def train(params):\n",
    "    torch.manual_seed(params['seed'])\n",
    "    np.random.seed(params['seed'])\n",
    "    \n",
    "    if params['optimizer'] == 'ES':\n",
    "        params['beta'] = 0 # ie no WD term in the blackbox function F\n",
    "    else:\n",
    "        params['beta'] = 0.5 # we use equal weight for the reward and WD. Other schemes could be tested...\n",
    "    \n",
    "    env = gym.make(params['env_name'])\n",
    "    params['ob_dim'] = env.observation_space.shape[0]\n",
    "    params['ac_dim'] = env.action_space.shape[0]\n",
    "\n",
    "    m = 0\n",
    "    v = 0\n",
    "        \n",
    "    params['zeros'] = False\n",
    "    master = ToeplitzPolicy(params)\n",
    "    print (master.N // params['sensings'])\n",
    "    \n",
    "    master.wass = random_wassdist(params) # initialize the behavioral test functions\n",
    "    master.buffer = []\n",
    "    master.embedding = []\n",
    "    \n",
    "    # KWNG\n",
    "    criterion = ES_criterion(beta=params['beta'])\n",
    "\n",
    "    device = \"cuda\" if tr.cuda.is_available() else \"cpu\"\n",
    "    wrapped_optimizer = get_wrapped_optimizer(params,\n",
    "                                      criterion,\n",
    "                                      master,\n",
    "                                      device=device)\n",
    "        \n",
    "    n_iter = 1\n",
    "    ts_cumulative = 0\n",
    "    ts = []\n",
    "    rewards = []\n",
    "    all_weights = pd.DataFrame()\n",
    "    start_time = time.time()\n",
    "    clock_ts = [] \n",
    "    \n",
    "    while n_iter < params['max_iter'] + 1:\n",
    "            \n",
    "        params['n_iter'] = n_iter\n",
    "        \n",
    "        # main calculations\n",
    "        gradient, reward, wdists, outputs, noise, noise_dist, timesteps = ES(params, master) \n",
    "        \n",
    "        ts_cumulative += timesteps\n",
    "        ts.append(ts_cumulative)\n",
    "        \n",
    "        # modify the gradients, if desired, via the wrapper\n",
    "        noisy_params = master.params.view(1, -1) + noise\n",
    "        gradient = wrapped_optimizer.step(\n",
    "            gradient, reward, wdists, outputs, noisy_params, noise_dist, params['sigma'], params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # update policy\n",
    "        gradient /= (torch.norm(gradient) / master.N + 1e-8)\n",
    "        update, m, v = Adam(gradient, m, v, params['learning_rate'], n_iter)\n",
    "        master.update(update)\n",
    "        \n",
    "        # evaluate new policy, keep the PPE to repel the next iteration\n",
    "        test_policy = worker(params, master, np.zeros([1, master.N]), 0)\n",
    "        reward, master_traj = test_policy.rollout()\n",
    "        master.embedding.append(embed(master_traj)) #params,\n",
    "        master.embedding = master.embedding[-params['n_prev']:]\n",
    "        rewards.append(reward)\n",
    "        clock_ts.append(time.time() - start_time)\n",
    "        \n",
    "        if n_iter % 1 == 0:\n",
    "            print('Iteration: %s/%s, Reward: %s, tot. time: %s' % \\\n",
    "                  (n_iter, params['max_iter'], reward, clock_ts[-1]/60.))\n",
    "        \n",
    "        if n_iter % 5 == 0 and params['save']:\n",
    "            ns = params['num_sensings'] \n",
    "            path = f\"./checkpts/{params['optimizer']}_{params['estimator']}_{ns}ns_{params['env_name']}_{params['seed']}_\"\n",
    "            np.savetxt(path + \"params.csv\", master.params.detach().numpy(), delimiter=',')\n",
    "            np.savetxt(path + \"adam_m.csv\", m.detach().numpy(), delimiter=',')\n",
    "            np.savetxt(path + \"adam_v.csv\", v.detach().numpy(), delimiter=',')\n",
    "            np.savetxt(path + \"embeddings.csv\", tr.stack(master.embedding).detach().numpy(), delimiter=',')\n",
    "            df = pd.DataFrame({\n",
    "                'Optimizer': [params['optimizer'] for _ in range(len(rewards))],\n",
    "                'Reward': rewards,\n",
    "                'Timesteps': ts,\n",
    "                'Clock_time': clock_ts\n",
    "            })\n",
    "            clip = \"Clip\" if params['clip_grad'] else \"\" #_{ns}ns\n",
    "            f = f\"./saved/{params['optimizer']}_{params['estimator']}{clip}_{ns}ns_antwall-v0_{params['seed']}.pkl\"\n",
    "            df.to_pickle(f)\n",
    "          \n",
    "        n_iter += 1\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        'Optimizer': [params['optimizer'] for _ in range(len(rewards))],\n",
    "        'Reward': rewards,\n",
    "        'Timesteps': ts,\n",
    "        'Clock_time': clock_ts\n",
    "    })\n",
    "    return(df, rewards, ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['env_name'] = 'antwall-v0'\n",
    "params['max_iter'] = 200 # number of iterations, >100 is usually enough\n",
    "params['sensings'] = 2 # population size = num params / sensings\n",
    "params['sigma'] = 0.12 # scale for the samples\n",
    "params['h_dim'] = 64 # size of neural network hidden layers\n",
    "params['rf_dim'] = 400 # dimensionality of random features, higher = more accurate but slower\n",
    "params['dim'] = params['rf_dim']\n",
    "params['w_iter'] = 100 # number of samples to update behavioral test funcs\n",
    "params['n_prev'] = 2 # number of previous policies to compute WD vs current. We have an ablation of this in Fig\n",
    "################# WNG stuff #####################\n",
    "params['epsilon'] = 1e-5\n",
    "params['num_basis'] = 5 # cannot be more than 2x population size \n",
    "params['with_diag_mat'] = 1\n",
    "params['dumping_freq'] = 5\n",
    "params['reduction_coeff'] = 0.85\n",
    "params['min_red'] = 0.25\n",
    "params['max_red'] = 0.75\n",
    "params['base_optimizer'] = 'sgd'\n",
    "params['kernel'] = 'gaussian'\n",
    "params['log_bandwidth'] = 0.\n",
    "params['momentum'] = 0.\n",
    "params['weight_decay'] = 0.\n",
    "params['basis_schedule'] = {} #{30: 1, 60: 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "KWNG beta = -1.3\n",
      "sigma:   28638.94140625\n",
      "Iteration: 1/200, Reward: -6102.391999192827, tot. time: 2.759707430998484\n",
      "sigma:   54340.3203125\n",
      "Iteration: 2/200, Reward: -6117.0509994583135, tot. time: 5.647935716311137\n",
      "sigma:   238140.0625\n",
      "Iteration: 3/200, Reward: -6075.880726920412, tot. time: 8.430505279699961\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-4364b2e496ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-e4264d80f03a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# main calculations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwdists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mts_cumulative\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-a078a242075f>\u001b[0m in \u001b[0;36mES\u001b[0;34m(params, master)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mnoisy_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwdists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-a078a242075f>\u001b[0m in \u001b[0;36maggregate_rollouts\u001b[0;34m(master, A, params)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_sensings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mall_rollouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-1d5cfe2925f0>\u001b[0m in \u001b[0;36mdo_rollouts\u001b[0;34m(self, seed, train)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Subtract the perturbation, to calculate F(theta - sigma * epsilon)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-1d5cfe2925f0>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(self, seed, train)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0maction_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OML/wng_po/WNPG/simpleenvs/antwall.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0maction_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_range\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_center\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print(action, action_scaled)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnext_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mqpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_body_com\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torso\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OML/wng_po/WNPG/simpleenvs/mujoco_env.py\u001b[0m in \u001b[0;36mdo_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctrl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params['optimizer'] = 'ES' # BGES or ES\n",
    "params['estimator'] = 'KWNG' # KWNG or EuclideanGradient\n",
    "params['save'] = False\n",
    "params['kwng_beta'] = -1.3 \n",
    "params['clip_grad'] = False\n",
    "params['num_sensings'] = 251\n",
    "params['seed'] = 1 \n",
    "params['learning_rate'] = 0.02\n",
    "df, rewards, timesteps = train(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['optimizer'] = 'ES'\n",
    "params['estimator'] = 'EuclideanGradient'\n",
    "params['save'] = False\n",
    "params['kwng_beta'] = -1.3 \n",
    "params['clip_grad'] = False\n",
    "params['num_sensings'] = 251\n",
    "params['seed'] = 1 # 2 is slow for KWNG w/ 1.3\n",
    "params['learning_rate'] = 0.02\n",
    "\n",
    "all_rewards = np.zeros([5, 200])\n",
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    print (\"==========================================\")\n",
    "    print (f\"training seed {seed}\")\n",
    "    params['seed'] = seed\n",
    "    df, rewards, timesteps = train(params)\n",
    "    all_rewards[seed, :] = np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorfill(x, y, yerr, color='C0', alpha_fill=0.3, alpha_line=1.0, ax=None, label=None, lw=1):\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "    if np.isscalar(yerr) or len(yerr) == len(y):\n",
    "        ymin = y - yerr\n",
    "        ymax = y + yerr\n",
    "    elif len(yerr) == 2:\n",
    "        ymin, ymax = yerr\n",
    "    ax.plot(x, y, color=color, label=label, lw=lw, alpha=alpha_line)\n",
    "    #ax.set_ylabel(r'fitness')\n",
    "    #ax.set_xlabel(r'evaluations')\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    ax.grid(alpha=0.7)\n",
    "    ax.legend(fontsize=22)\n",
    "    ax.fill_between(x, ymax, ymin, color=color, alpha=alpha_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.Timesteps, df.Reward, label='ES')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
